{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68185bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Attention Mechanism: Attention × Value = Output\n",
      "======================================================================\n",
      "\n",
      "1. Attention Weights (3×3):\n",
      "   Rows: Query tokens ['Me', 'against', 'the']\n",
      "   Cols: Key tokens ['Me', 'against', 'the']\n",
      "\n",
      "[[0.7 0.2 0.1]\n",
      " [0.3 0.5 0.2]\n",
      " [0.1 0.3 0.6]]\n",
      "\n",
      "   Interpretation:\n",
      "   - Row 0: 'Me' pays 70% attention to 'Me', 20% to 'against', 10% to 'the'\n",
      "   - Row 1: 'against' pays 30% to 'Me', 50% to 'against', 20% to 'the'\n",
      "   - Row 2: 'the' pays 10% to 'Me', 30% to 'against', 60% to 'the'\n",
      "\n",
      "2. Value Matrix V (3×64):\n",
      "   Each row is the 64-dim value vector for a token\n",
      "\n",
      "[[ 0.5  -0.14  0.65  1.52 -0.23 -0.23  1.58  0.77 -0.47  0.54]\n",
      " [ 0.81  1.36 -0.07  1.    0.36 -0.65  0.36  1.54 -0.04  1.56]\n",
      " [ 0.1  -0.5  -1.55  0.07 -1.06  0.47 -0.92  1.55 -0.78 -0.32]]  ... (showing first 10 of 64 dimensions)\n",
      "\n",
      "3. Output = Attention × Value (3×64):\n",
      "   Each row is the weighted sum of value vectors\n",
      "\n",
      "[[ 0.52  0.12  0.28  1.27 -0.2  -0.25  1.09  1.   -0.41  0.66]\n",
      " [ 0.58  0.54 -0.15  0.97 -0.1  -0.3   0.47  1.31 -0.32  0.88]\n",
      " [ 0.35  0.09 -0.89  0.49 -0.55  0.07 -0.29  1.47 -0.53  0.33]]  ... (showing first 10 of 64 dimensions)\n",
      "\n",
      "======================================================================\n",
      "DETAILED BREAKDOWN: How output for 'Me' (first token) is computed\n",
      "======================================================================\n",
      "\n",
      "Output[0] = weighted sum of all value vectors:\n",
      "  = 0.7 × V[0] (Me)\n",
      "  + 0.2 × V[1] (against)\n",
      "  + 0.1 × V[2] (the)\n",
      "\n",
      "First 10 dimensions of Output[0]:\n",
      "  Computed: [ 0.52   0.124  0.284  1.274 -0.198 -0.246  1.086  1.    -0.414  0.661]\n",
      "  Manual:   [ 0.52   0.124  0.284  1.274 -0.198 -0.246  1.086  1.    -0.414  0.661]\n",
      "  Match: True\n",
      "\n",
      "======================================================================\n",
      "MINI EXAMPLE: With d_v=4 for easy visualization\n",
      "======================================================================\n",
      "\n",
      "Value Matrix (3×4):\n",
      "[[1.  2.  3.  4. ]\n",
      " [0.5 1.5 2.5 3.5]\n",
      " [2.  1.  4.  3. ]]\n",
      "\n",
      "Attention Weights (3×3):\n",
      "[[0.7 0.2 0.1]\n",
      " [0.3 0.5 0.2]\n",
      " [0.1 0.3 0.6]]\n",
      "\n",
      "Output = Attention × Value (3×4):\n",
      "[[1.   1.8  3.   3.8 ]\n",
      " [0.95 1.55 2.95 3.55]\n",
      " [1.45 1.25 3.45 3.25]]\n",
      "\n",
      "\n",
      "Manual calculation for Output[0] (token 'Me'):\n",
      "  Output[0] = 0.7×[1.0, 2.0, 3.0, 4.0]\n",
      "            + 0.2×[0.5, 1.5, 2.5, 3.5]\n",
      "            + 0.1×[2.0, 1.0, 4.0, 3.0]\n",
      "            = [1.0, 1.8, 3.0, 3.8]\n",
      "\n",
      "======================================================================\n",
      "MATHEMATICAL FORMULA\n",
      "======================================================================\n",
      "\n",
      "For each output position i:\n",
      "  Output_i = Σ(k=1 to seq_length) Attention_ik × Value_k\n",
      "\n",
      "Where:\n",
      "  - Attention_ik = how much position i attends to position k\n",
      "  - Value_k = the value vector at position k\n",
      "  - The sum creates a weighted combination of all value vectors\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPLETE ATTENTION FLOW: Q, K, V → Output\n",
      "======================================================================\n",
      "\n",
      "Step 1: Compute attention scores (QK^T)\n",
      "Scores shape: (3, 3)\n",
      "[[1. 1. 2.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Step 2: Scale by √d_k\n",
      "[[0.5 0.5 1. ]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n",
      "\n",
      "Step 3: Apply softmax to get attention weights\n",
      "[[0.274 0.274 0.452]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]]\n",
      "\n",
      "Step 4: Multiply attention weights by Value matrix\n",
      "Output shape: (3, 4)\n",
      "[[1.31 1.41 3.31 3.41]\n",
      " [1.17 1.5  3.17 3.5 ]\n",
      " [1.17 1.5  3.17 3.5 ]]\n",
      "\n",
      "✓ This is the complete Scaled Dot-Product Attention!\n",
      "  Formula: Attention(Q, K, V) = softmax(QK^T / √d_k) × V\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attention Mechanism: Attention × Value = Output\n",
    "=================================================\n",
    "This demonstrates how attention weights are multiplied by the Value matrix\n",
    "to produce the final output, as shown in the diagram.\n",
    "\n",
    "The example uses 3 tokens: [\"Me\", \"against\", \"the\"]\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def attention_value_example():\n",
    "    \"\"\"Demonstrate Attention × Value = Output\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Attention Mechanism: Attention × Value = Output\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Setup\n",
    "    seq_length = 3  # 3 tokens: \"Me\", \"against\", \"the\"\n",
    "    d_v = 64  # Value dimension (matching d_k typically)\n",
    "    \n",
    "    # Step 1: Attention weights (after softmax of QK^T / √d_k)\n",
    "    # Shape: (seq_length, seq_length) = (3, 3)\n",
    "    # Each row shows how much each token attends to all tokens\n",
    "    attention_weights = np.array([\n",
    "        [0.7, 0.2, 0.1],   # \"Me\" attends mostly to itself\n",
    "        [0.3, 0.5, 0.2],   # \"against\" attends to all three\n",
    "        [0.1, 0.3, 0.6]    # \"the\" attends mostly to itself\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n1. Attention Weights (3×3):\")\n",
    "    print(\"   Rows: Query tokens ['Me', 'against', 'the']\")\n",
    "    print(\"   Cols: Key tokens ['Me', 'against', 'the']\")\n",
    "    print(f\"\\n{attention_weights}\")\n",
    "    print(\"\\n   Interpretation:\")\n",
    "    print(\"   - Row 0: 'Me' pays 70% attention to 'Me', 20% to 'against', 10% to 'the'\")\n",
    "    print(\"   - Row 1: 'against' pays 30% to 'Me', 50% to 'against', 20% to 'the'\")\n",
    "    print(\"   - Row 2: 'the' pays 10% to 'Me', 30% to 'against', 60% to 'the'\")\n",
    "    \n",
    "    # Step 2: Value matrix\n",
    "    # Shape: (seq_length, d_v) = (3, 64)\n",
    "    # Each row is the value representation for a token\n",
    "    np.random.seed(42)\n",
    "    V = np.random.randn(seq_length, d_v)\n",
    "    \n",
    "    print(f\"\\n2. Value Matrix V (3×64):\")\n",
    "    print(\"   Each row is the 64-dim value vector for a token\")\n",
    "    print(f\"\\n{V[:, :10].round(2)}  ... (showing first 10 of 64 dimensions)\")\n",
    "    \n",
    "    # Step 3: Compute Output = Attention × Value\n",
    "    # Shape: (3, 3) × (3, 64) = (3, 64)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    print(f\"\\n3. Output = Attention × Value (3×64):\")\n",
    "    print(\"   Each row is the weighted sum of value vectors\")\n",
    "    print(f\"\\n{output[:, :10].round(2)}  ... (showing first 10 of 64 dimensions)\")\n",
    "    \n",
    "    # Detailed breakdown for first token\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED BREAKDOWN: How output for 'Me' (first token) is computed\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nOutput[0] = weighted sum of all value vectors:\")\n",
    "    print(f\"  = {attention_weights[0, 0]:.1f} × V[0] (Me)\")\n",
    "    print(f\"  + {attention_weights[0, 1]:.1f} × V[1] (against)\")\n",
    "    print(f\"  + {attention_weights[0, 2]:.1f} × V[2] (the)\")\n",
    "    \n",
    "    # Manual calculation for verification\n",
    "    manual_output_0 = (attention_weights[0, 0] * V[0] + \n",
    "                       attention_weights[0, 1] * V[1] + \n",
    "                       attention_weights[0, 2] * V[2])\n",
    "    \n",
    "    print(f\"\\nFirst 10 dimensions of Output[0]:\")\n",
    "    print(f\"  Computed: {output[0, :10].round(3)}\")\n",
    "    print(f\"  Manual:   {manual_output_0[:10].round(3)}\")\n",
    "    print(f\"  Match: {np.allclose(output[0], manual_output_0)}\")\n",
    "    \n",
    "    # Complete example with mini dimensions for visualization\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MINI EXAMPLE: With d_v=4 for easy visualization\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Smaller example for clarity\n",
    "    V_mini = np.array([\n",
    "        [1.0, 2.0, 3.0, 4.0],    # Value for \"Me\"\n",
    "        [0.5, 1.5, 2.5, 3.5],    # Value for \"against\"\n",
    "        [2.0, 1.0, 4.0, 3.0]     # Value for \"the\"\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nValue Matrix (3×4):\")\n",
    "    print(V_mini)\n",
    "    \n",
    "    print(\"\\nAttention Weights (3×3):\")\n",
    "    print(attention_weights)\n",
    "    \n",
    "    output_mini = attention_weights @ V_mini\n",
    "    \n",
    "    print(\"\\nOutput = Attention × Value (3×4):\")\n",
    "    print(output_mini.round(2))\n",
    "    \n",
    "    print(\"\\n\\nManual calculation for Output[0] (token 'Me'):\")\n",
    "    print(f\"  Output[0] = 0.7×[1.0, 2.0, 3.0, 4.0]\")\n",
    "    print(f\"            + 0.2×[0.5, 1.5, 2.5, 3.5]\")\n",
    "    print(f\"            + 0.1×[2.0, 1.0, 4.0, 3.0]\")\n",
    "    print(f\"            = [{output_mini[0, 0]:.1f}, {output_mini[0, 1]:.1f}, {output_mini[0, 2]:.1f}, {output_mini[0, 3]:.1f}]\")\n",
    "    \n",
    "    # Summary with equation\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MATHEMATICAL FORMULA\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nFor each output position i:\")\n",
    "    print(\"  Output_i = Σ(k=1 to seq_length) Attention_ik × Value_k\")\n",
    "    print(\"\\nWhere:\")\n",
    "    print(\"  - Attention_ik = how much position i attends to position k\")\n",
    "    print(\"  - Value_k = the value vector at position k\")\n",
    "    print(\"  - The sum creates a weighted combination of all value vectors\")\n",
    "\n",
    "def complete_attention_flow():\n",
    "    \"\"\"Show the complete attention mechanism from Q, K, V to Output\"\"\"\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\" * 70)\n",
    "    print(\"COMPLETE ATTENTION FLOW: Q, K, V → Output\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    seq_length = 3\n",
    "    d_k = 4  # dimension\n",
    "    \n",
    "    # Input representations\n",
    "    Q = np.array([\n",
    "        [1.0, 0.0, 1.0, 0.0],\n",
    "        [0.0, 1.0, 0.0, 1.0],\n",
    "        [1.0, 1.0, 0.0, 0.0]\n",
    "    ])\n",
    "    \n",
    "    K = np.array([\n",
    "        [1.0, 0.0, 0.0, 1.0],\n",
    "        [0.0, 1.0, 1.0, 0.0],\n",
    "        [1.0, 0.0, 1.0, 1.0]\n",
    "    ])\n",
    "    \n",
    "    V = np.array([\n",
    "        [1.0, 2.0, 3.0, 4.0],\n",
    "        [0.5, 1.5, 2.5, 3.5],\n",
    "        [2.0, 1.0, 4.0, 3.0]\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nStep 1: Compute attention scores (QK^T)\")\n",
    "    scores = Q @ K.T\n",
    "    print(f\"Scores shape: {scores.shape}\")\n",
    "    print(scores)\n",
    "    \n",
    "    print(\"\\nStep 2: Scale by √d_k\")\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    print(scaled_scores.round(2))\n",
    "    \n",
    "    print(\"\\nStep 3: Apply softmax to get attention weights\")\n",
    "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=1, keepdims=True))\n",
    "    attention = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    print(attention.round(3))\n",
    "    \n",
    "    print(\"\\nStep 4: Multiply attention weights by Value matrix\")\n",
    "    output = attention @ V\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(output.round(2))\n",
    "    \n",
    "    print(\"\\n✓ This is the complete Scaled Dot-Product Attention!\")\n",
    "    print(\"  Formula: Attention(Q, K, V) = softmax(QK^T / √d_k) × V\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attention_value_example()\n",
    "    complete_attention_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05153151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Example 1: Basic QK^T Operation\n",
      "==================================================\n",
      "\n",
      "Matrix Q (2x3):\n",
      "[[1 2 3]\n",
      " [1 2 1]]\n",
      "\n",
      "Matrix K (2x3):\n",
      "[[1 2 1]\n",
      " [2 1 3]]\n",
      "\n",
      "Matrix K^T (3x2):\n",
      "[[1 2]\n",
      " [2 1]\n",
      " [1 3]]\n",
      "\n",
      "Result of QK^T (2x2):\n",
      "[[ 8 13]\n",
      " [ 6  7]]\n",
      "\n",
      "==================================================\n",
      "Example 2: Attention Mechanism (Self-Attention)\n",
      "==================================================\n",
      "\n",
      "Query matrix Q (4x3):\n",
      "[[-0.07 -1.43 -0.21]\n",
      " [ 0.13  1.43  0.9 ]\n",
      " [ 0.05 -0.58 -1.57]\n",
      " [-0.84  0.49 -0.58]]\n",
      "\n",
      "Key matrix K (4x3):\n",
      "[[-0.11 -0.04  0.79]\n",
      " [-1.68 -0.42  0.59]\n",
      " [-0.59  0.77 -0.67]\n",
      " [ 0.2   0.02  0.16]]\n",
      "\n",
      "Attention scores (QK^T) (4x4):\n",
      "[[-0.11  0.59 -0.92 -0.08]\n",
      " [ 0.65 -0.3   0.43  0.2 ]\n",
      " [-1.23 -0.76  0.57 -0.25]\n",
      " [-0.38  0.87  1.27 -0.25]]\n",
      "\n",
      "Scaled attention scores (QK^T / √d_k):\n",
      "[[-0.06  0.34 -0.53 -0.05]\n",
      " [ 0.38 -0.17  0.25  0.12]\n",
      " [-0.71 -0.44  0.33 -0.14]\n",
      " [-0.22  0.5   0.73 -0.14]]\n",
      "\n",
      "Attention weights (softmax of scaled scores):\n",
      "[[0.241 0.362 0.151 0.246]\n",
      " [0.309 0.179 0.273 0.239]\n",
      " [0.145 0.191 0.409 0.255]\n",
      " [0.149 0.306 0.385 0.16 ]]\n",
      "\n",
      "==================================================\n",
      "Example 3: Different Ways to Compute QK^T\n",
      "==================================================\n",
      "\n",
      "Method 1: Using @ operator\n",
      "[[17 23]\n",
      " [39 53]]\n",
      "\n",
      "Method 2: Using np.matmul()\n",
      "[[17 23]\n",
      " [39 53]]\n",
      "\n",
      "Method 3: Using np.dot()\n",
      "[[17 23]\n",
      " [39 53]]\n",
      "\n",
      "All methods produce the same result: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QK^T Matrix Operation Example\n",
    "==============================\n",
    "This demonstrates the matrix multiplication of Q and K transpose,\n",
    "commonly used in attention mechanisms (e.g., Transformer models).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def qk_transpose_operation():\n",
    "    \"\"\"Demonstrate Q * K^T matrix operation\"\"\"\n",
    "    \n",
    "    # Example 1: Simple 2x3 matrices\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Example 1: Basic QK^T Operation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    Q = np.array([\n",
    "        [1, 2, 3],\n",
    "        [1, 2, 1],\n",
    "    ])\n",
    "    \n",
    "    K = np.array([\n",
    "        [1, 2, 1],\n",
    "        [2, 1, 3],\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nMatrix Q (2x3):\")\n",
    "    print(Q)\n",
    "    \n",
    "    print(\"\\nMatrix K (2x3):\")\n",
    "    print(K)\n",
    "    \n",
    "    print(\"\\nMatrix K^T (3x2):\")\n",
    "    K_transpose = K.T\n",
    "    print(K_transpose)\n",
    "    \n",
    "    print(\"\\nResult of QK^T (2x2):\")\n",
    "    result = Q @ K_transpose  # or np.matmul(Q, K.T)\n",
    "    print(result)\n",
    "    \n",
    "    # Example 2: Attention mechanism context\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Example 2: Attention Mechanism (Self-Attention)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Simulating queries, keys with sequence length 4, embedding dimension 3\n",
    "    seq_length = 4\n",
    "    d_k = 3  # key/query dimension\n",
    "    \n",
    "    Q_attention = np.random.randn(seq_length, d_k)\n",
    "    K_attention = np.random.randn(seq_length, d_k)\n",
    "    \n",
    "    print(f\"\\nQuery matrix Q ({seq_length}x{d_k}):\")\n",
    "    print(Q_attention.round(2))\n",
    "    \n",
    "    print(f\"\\nKey matrix K ({seq_length}x{d_k}):\")\n",
    "    print(K_attention.round(2))\n",
    "    \n",
    "    print(f\"\\nAttention scores (QK^T) ({seq_length}x{seq_length}):\")\n",
    "    attention_scores = Q_attention @ K_attention.T\n",
    "    print(attention_scores.round(2))\n",
    "    \n",
    "    # Scaled attention scores (as in Transformer)\n",
    "    print(f\"\\nScaled attention scores (QK^T / √d_k):\")\n",
    "    scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "    print(scaled_scores.round(2))\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    print(f\"\\nAttention weights (softmax of scaled scores):\")\n",
    "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=1, keepdims=True))\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    print(attention_weights.round(3))\n",
    "    \n",
    "    # Example 3: Different methods to compute QK^T\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Example 3: Different Ways to Compute QK^T\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    Q_test = np.array([[1, 2], [3, 4]])\n",
    "    K_test = np.array([[5, 6], [7, 8]])\n",
    "    \n",
    "    print(\"\\nMethod 1: Using @ operator\")\n",
    "    result1 = Q_test @ K_test.T\n",
    "    print(result1)\n",
    "    \n",
    "    print(\"\\nMethod 2: Using np.matmul()\")\n",
    "    result2 = np.matmul(Q_test, K_test.T)\n",
    "    print(result2)\n",
    "    \n",
    "    print(\"\\nMethod 3: Using np.dot()\")\n",
    "    result3 = np.dot(Q_test, K_test.T)\n",
    "    print(result3)\n",
    "    \n",
    "    print(\"\\nAll methods produce the same result:\", \n",
    "          np.allclose(result1, result2) and np.allclose(result2, result3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qk_transpose_operation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
